<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>注意力机制QKV历史发展</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        
        body {
            background: linear-gradient(135deg, #1a2a6c, #b21f1f, #1a2a6c);
            color: #f0f0f0;
            min-height: 100vh;
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            padding: 40px 20px;
            background: rgba(0, 0, 0, 0.6);
            border-radius: 20px;
            margin-bottom: 40px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.4);
            border: 1px solid rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
        }
        
        h1 {
            font-size: 3.5rem;
            margin-bottom: 15px;
            background: linear-gradient(to right, #ff7e5f, #feb47b, #86a8e7, #91eae4);
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
            text-shadow: 0 2px 10px rgba(0, 0, 0, 0.2);
        }
        
        .subtitle {
            font-size: 1.4rem;
            max-width: 800px;
            margin: 0 auto;
            line-height: 1.6;
            color: #d0d0ff;
        }
        
        .qkv-explanation {
            display: flex;
            justify-content: center;
            gap: 30px;
            flex-wrap: wrap;
            margin: 40px 0;
        }
        
        .qkv-card {
            background: rgba(20, 30, 80, 0.8);
            border-radius: 15px;
            padding: 25px;
            width: 300px;
            text-align: center;
            transition: transform 0.3s, box-shadow 0.3s;
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.3);
            border: 1px solid rgba(100, 150, 255, 0.3);
        }
        
        .qkv-card:hover {
            transform: translateY(-10px);
            box-shadow: 0 15px 35px rgba(0, 0, 0, 0.5);
        }
        
        .qkv-card h2 {
            font-size: 2.2rem;
            margin-bottom: 15px;
        }
        
        .q-card h2 { color: #ff7e5f; }
        .k-card h2 { color: #86a8e7; }
        .v-card h2 { color: #91eae4; }
        
        .qkv-card p {
            font-size: 1.1rem;
            line-height: 1.6;
            color: #c0c0ff;
        }
        
        .timeline {
            position: relative;
            max-width: 1000px;
            margin: 50px auto;
            padding: 40px 0;
        }
        
        .timeline::after {
            content: '';
            position: absolute;
            width: 6px;
            background: linear-gradient(to bottom, #ff7e5f, #86a8e7, #91eae4);
            top: 0;
            bottom: 0;
            left: 50%;
            margin-left: -3px;
            border-radius: 10px;
        }
        
        .timeline-item {
            padding: 20px 40px;
            position: relative;
            width: 50%;
            box-sizing: border-box;
        }
        
        .timeline-item:nth-child(odd) {
            left: 0;
        }
        
        .timeline-item:nth-child(even) {
            left: 50%;
        }
        
        .timeline-content {
            padding: 30px;
            background: rgba(20, 25, 60, 0.85);
            border-radius: 15px;
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.4);
            transition: transform 0.3s;
            border: 1px solid rgba(100, 150, 255, 0.2);
            backdrop-filter: blur(5px);
        }
        
        .timeline-content:hover {
            transform: scale(1.03);
        }
        
        .timeline-content h3 {
            font-size: 1.8rem;
            margin-bottom: 15px;
            color: #feb47b;
        }
        
        .timeline-content .year {
            display: inline-block;
            background: linear-gradient(to right, #ff7e5f, #feb47b);
            padding: 5px 15px;
            border-radius: 20px;
            font-weight: bold;
            margin-bottom: 15px;
        }
        
        .timeline-content p {
            line-height: 1.7;
            color: #d0d0ff;
            margin-bottom: 15px;
        }
        
        .timeline-content .authors {
            font-style: italic;
            color: #86a8e7;
            margin-top: 10px;
        }
        
        .timeline-item:nth-child(odd)::after {
            content: '';
            position: absolute;
            width: 20px;
            height: 20px;
            right: -10px;
            background: #ff7e5f;
            border: 4px solid #1a2a6c;
            top: 40px;
            border-radius: 50%;
            z-index: 1;
        }
        
        .timeline-item:nth-child(even)::after {
            content: '';
            position: absolute;
            width: 20px;
            height: 20px;
            left: -10px;
            background: #86a8e7;
            border: 4px solid #1a2a6c;
            top: 40px;
            border-radius: 50%;
            z-index: 1;
        }
        
        .diagram-section {
            background: rgba(10, 15, 40, 0.9);
            border-radius: 20px;
            padding: 40px;
            margin: 60px 0;
            text-align: center;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.5);
            border: 1px solid rgba(100, 150, 255, 0.2);
        }
        
        .diagram-section h2 {
            font-size: 2.5rem;
            margin-bottom: 40px;
            color: #91eae4;
            text-align: center;
        }
        
        .attention-diagram {
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            gap: 30px;
            margin: 30px 0;
        }
        
        .diagram-element {
            background: rgba(30, 40, 90, 0.8);
            padding: 25px;
            border-radius: 15px;
            min-width: 200px;
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.4);
            border: 1px solid rgba(100, 150, 255, 0.3);
        }
        
        .diagram-element h3 {
            font-size: 1.5rem;
            margin-bottom: 15px;
        }
        
        .arrow {
            font-size: 2.5rem;
            color: #feb47b;
        }
        
        .formula {
            background: rgba(0, 0, 0, 0.4);
            padding: 20px;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            font-size: 1.4rem;
            margin: 30px auto;
            max-width: 600px;
            text-align: center;
            color: #feb47b;
            border: 1px solid #86a8e7;
        }
        
        footer {
            text-align: center;
            padding: 30px;
            margin-top: 50px;
            background: rgba(0, 0, 0, 0.6);
            border-radius: 15px;
            color: #c0c0ff;
            font-size: 1.1rem;
        }
        
        .highlight {
            color: #ffb347;
            font-weight: bold;
        }
        
        @media (max-width: 768px) {
            .timeline::after {
                left: 31px;
            }
            
            .timeline-item {
                width: 100%;
                padding-left: 70px;
                padding-right: 25px;
            }
            
            .timeline-item:nth-child(even) {
                left: 0;
            }
            
            .timeline-item:nth-child(odd)::after,
            .timeline-item:nth-child(even)::after {
                left: 21px;
            }
            
            h1 {
                font-size: 2.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1><i class="fas fa-brain"></i> 注意力机制中的 QKV 历史</h1>
            <p class="subtitle">探索注意力机制中Query(查询)、Key(键)、Value(值)概念的演变与发展历程，从早期思想到Transformer革命</p>
        </header>
        
        <div class="qkv-explanation">
            <div class="qkv-card q-card">
                <h2><i class="fas fa-search"></i> Query</h2>
                <p>表示当前需要关注的内容。在注意力机制中，Query通常是解码器当前时刻的隐藏状态，用于查询编码器的信息。</p>
            </div>
            
            <div class="qkv-card k-card">
                <h2><i class="fas fa-key"></i> Key</h2>
                <p>表示被查询的内容特征。Key通常来自编码器的所有隐藏状态，用于与Query计算相似度。</p>
            </div>
            
            <div class="qkv-card v-card">
                <h2><i class="fas fa-database"></i> Value</h2>
                <p>表示实际的特征值。在计算注意力权重后，Value会被加权求和得到最终的上下文向量。</p>
            </div>
        </div>
        
        <div class="timeline">
            <div class="timeline-item">
                <div class="timeline-content">
                    <div class="year">2014</div>
                    <h3>序列到序列模型</h3>
                    <p>Sutskever等人提出Seq2Seq模型，使用编码器-解码器框架处理序列任务。但模型使用固定长度的上下文向量，限制了处理长序列的能力。</p>
                    <p class="authors">Sutskever, I., Vinyals, O., & Le, Q. V.</p>
                </div>
            </div>
            
            <div class="timeline-item">
                <div class="timeline-content">
                    <div class="year">2015</div>
                    <h3>神经机器翻译的注意力机制</h3>
                    <p>Bahdanau等人首次在神经机器翻译中引入注意力机制，允许解码器访问编码器的所有隐藏状态。这被视为<span class="highlight">QKV概念的雏形</span>。</p>
                    <p class="authors">Bahdanau, D., Cho, K., & Bengio, Y.</p>
                </div>
            </div>
            
            <div class="timeline-item">
                <div class="timeline-content">
                    <div class="year">2015</div>
                    <h3>全局与局部注意力</h3>
                    <p>Luong等人提出改进的注意力机制，区分全局注意力和局部注意力，并引入不同的对齐函数。他们首次明确使用<span class="highlight">"query"和"key"</span>的术语。</p>
                    <p class="authors">Luong, M. T., Pham, H., & Manning, C. D.</p>
                </div>
            </div>
            
            <div class="timeline-item">
                <div class="timeline-content">
                    <div class="year">2017</div>
                    <h3>Transformer与自注意力</h3>
                    <p>Vaswani等人提出Transformer模型，完全基于注意力机制。<span class="highlight">正式引入Q(Query)、K(Key)、V(Value)概念</span>，并推广自注意力机制。</p>
                    <p class="authors">Vaswani, A., et al.</p>
                </div>
            </div>
            
            <div class="timeline-item">
                <div class="timeline-content">
                    <div class="year">2018</div>
                    <h3>BERT与预训练模型</h3>
                    <p>Devlin等人提出BERT模型，利用Transformer编码器和自注意力机制进行大规模预训练。QKV机制成为自然语言处理的<span class="highlight">核心组件</span>。</p>
                    <p class="authors">Devlin, J., et al.</p>
                </div>
            </div>
        </div>
        
        <div class="diagram-section">
            <h2><i class="fas fa-project-diagram"></i> 注意力机制中的QKV计算</h2>
            
            <div class="attention-diagram">
                <div class="diagram-element">
                    <h3>Query</h3>
                    <p>当前需要关注的表示</p>
                </div>
                
                <div class="arrow">
                    <i class="fas fa-times"></i>
                </div>
                
                <div class="diagram-element">
                    <h3>Key</h3>
                    <p>被查询的内容特征</p>
                </div>
                
                <div class="arrow">
                    <i class="fas fa-long-arrow-alt-right"></i>
                </div>
                
                <div class="diagram-element">
                    <h3>Softmax</h3>
                    <p>计算注意力权重</p>
                </div>
                
                <div class="arrow">
                    <i class="fas fa-long-arrow-alt-right"></i>
                </div>
                
                <div class="diagram-element">
                    <h3>Value</h3>
                    <p>加权求和得到输出</p>
                </div>
            </div>
            
            <div class="formula">
                Attention(Q, K, V) = softmax(QK<sup>T</sup>/√d<sub>k</sub>)V
            </div>
            
            <p>注意力机制的核心计算：通过Query和Key的相似度计算权重，然后对Value进行加权求和</p>
        </div>
        
        <footer>
            <p>© 2023 注意力机制QKV历史展示 | 深度学习和自然语言处理中的重要概念</p>
            <p>QKV机制已成为现代Transformer架构的核心，推动了GPT、BERT等革命性模型的发展</p>
        </footer>
    </div>

    <script>
        // 添加简单的滚动动画效果
        document.addEventListener('DOMContentLoaded', function() {
            const timelineItems = document.querySelectorAll('.timeline-content');
            
            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.style.animation = 'fadeInUp 0.8s ease-out forwards';
                    }
                });
            }, { threshold: 0.1 });
            
            timelineItems.forEach(item => {
                item.style.opacity = '0';
                item.style.transform = 'translateY(30px)';
                observer.observe(item);
            });
            
            // 添加CSS动画
            const style = document.createElement('style');
            style.textContent = `
                @keyframes fadeInUp {
                    from {
                        opacity: 0;
                        transform: translateY(30px);
                    }
                    to {
                        opacity: 1;
                        transform: translateY(0);
                    }
                }
            `;
            document.head.appendChild(style);
        });
    </script>
</body>
</html>