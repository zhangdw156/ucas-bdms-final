<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>注意力机制QKV历史演进</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.8/dist/chart.umd.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        primary: '#165DFF',
                        secondary: '#722ED1',
                        accent: '#0FC6C2',
                        neutral: '#F5F7FA',
                        dark: '#1D2129',
                        mha: '#FF7D00',
                    },
                    fontFamily: {
                        inter: ['Inter', 'sans-serif'],
                    },
                }
            }
        }
    </script>
    <style type="text/tailwindcss">
        @layer utilities {
            .content-auto {
                content-visibility: auto;
            }
            .timeline-line {
                @apply absolute top-0 bottom-0 left-9 w-0.5 bg-primary/20;
            }
            .timeline-dot {
                @apply absolute left-7 w-5 h-5 rounded-full border-4 border-white bg-primary shadow-md z-10;
            }
            .card-hover {
                @apply transition-all duration-300 hover:shadow-lg hover:-translate-y-1;
            }
            .animate-float {
                animation: float 6s ease-in-out infinite;
            }
            @keyframes float {
                0% { transform: translateY(0px); }
                50% { transform: translateY(-15px); }
                100% { transform: translateY(0px); }
            }
        }
    </style>
</head>
<body class="font-inter bg-neutral text-dark antialiased">
    <!-- 导航栏 -->
    <nav id="navbar" class="fixed w-full bg-white/90 backdrop-blur-md z-50 transition-all duration-300 shadow-sm">
        <div class="container mx-auto px-4 py-3 flex justify-between items-center">
            <div class="flex items-center space-x-2">
                <div class="w-10 h-10 rounded-lg bg-gradient-to-br from-primary to-secondary flex items-center justify-center">
                    <i class="fa fa-lightbulb-o text-white text-xl"></i>
                </div>
                <span class="text-xl font-bold bg-gradient-to-r from-primary to-secondary bg-clip-text text-transparent">注意力机制QKV</span>
            </div>
            <div class="hidden md:flex space-x-8 items-center">
                <a href="#overview" class="font-medium text-dark/80 hover:text-primary transition-colors">概述</a>
                <a href="#timeline" class="font-medium text-dark/80 hover:text-primary transition-colors">历史时间线</a>
                <a href="#components" class="font-medium text-dark/80 hover:text-primary transition-colors">QKV组件</a>
                <a href="#optimizations" class="font-medium text-dark/80 hover:text-primary transition-colors">优化技术</a>
                <a href="#applications" class="font-medium text-dark/80 hover:text-primary transition-colors">应用场景</a>
                <a href="#resources" class="font-medium text-dark/80 hover:text-primary transition-colors">学习资源</a>
                <button class="bg-primary hover:bg-primary/90 text-white px-5 py-2 rounded-lg transition-all duration-300 shadow-md hover:shadow-lg">
                    <i class="fa fa-download mr-2"></i>下载资料
                </button>
            </div>
            <button class="md:hidden text-dark text-xl">
                <i class="fa fa-bars"></i>
            </button>
        </div>
    </nav>

    <!-- 英雄区 -->
    <section class="pt-32 pb-20 bg-gradient-to-br from-primary/5 to-secondary/5 overflow-hidden relative">
        <div class="absolute top-0 right-0 w-1/3 h-full opacity-10">
            <div class="w-full h-full bg-[url('https://picsum.photos/id/180/800/800')] bg-cover bg-center"></div>
        </div>
        <div class="container mx-auto px-4 relative">
            <div class="max-w-4xl mx-auto text-center mb-16">
                <h1 class="text-[clamp(2.5rem,5vw,4rem)] font-bold leading-tight mb-6">
                    注意力机制<span class="bg-gradient-to-r from-primary to-secondary bg-clip-text text-transparent">QKV</span>的历史演进
                </h1>
                <p class="text-lg text-dark/70 mb-8 max-w-3xl mx-auto">
                    探索从早期理论到Transformer的发展历程，了解Query、Key和Value如何重塑AI领域，以及最新的优化技术如何提升其效率
                </p>
                <div class="flex flex-wrap justify-center gap-4">
                    <button class="bg-primary hover:bg-primary/90 text-white px-8 py-3 rounded-lg transition-all duration-300 shadow-lg hover:shadow-xl">
                        <i class="fa fa-play-circle mr-2"></i>观看演示
                    </button>
                    <button class="bg-white hover:bg-gray-50 text-primary border border-primary/20 px-8 py-3 rounded-lg transition-all duration-300 shadow-md hover:shadow-lg">
                        <i class="fa fa-book mr-2"></i>阅读指南
                    </button>
                </div>
            </div>
            
            <div class="flex justify-center mt-12">
                <div class="w-full max-w-6xl relative">
                    <div class="bg-white rounded-2xl shadow-2xl p-8 md:p-12">
                        <div class="grid grid-cols-1 md:grid-cols-3 gap-8">
                            <div class="flex flex-col items-center text-center p-6 rounded-xl bg-neutral">
                                <div class="w-16 h-16 rounded-full bg-primary/10 flex items-center justify-center mb-4">
                                    <i class="fa fa-history text-primary text-2xl"></i>
                                </div>
                                <h3 class="text-xl font-semibold mb-2">20+年历程</h3>
                                <p class="text-dark/70">从2002年首次提出到如今的广泛应用</p>
                            </div>
                            <div class="flex flex-col items-center text-center p-6 rounded-xl bg-secondary/10">
                                <div class="w-16 h-16 rounded-full bg-secondary/20 flex items-center justify-center mb-4">
                                    <i class="fa fa-file-text-o text-secondary text-2xl"></i>
                                </div>
                                <h3 class="text-xl font-semibold mb-2">500+论文</h3>
                                <p class="text-dark/70">关于注意力机制的研究文献</p>
                            </div>
                            <div class="flex flex-col items-center text-center p-6 rounded-xl bg-accent/10">
                                <div class="w-16 h-16 rounded-full bg-accent/20 flex items-center justify-center mb-4">
                                    <i class="fa fa-bolt text-accent text-2xl"></i>
                                </div>
                                <h3 class="text-xl font-semibold mb-2">10+优化</h3>
                                <p class="text-dark/70">从FlashAttention到MLP的效率提升</p>
                            </div>
                        </div>
                    </div>
                    
                    <!-- 装饰元素 -->
                    <div class="absolute -top-10 -left-10 w-20 h-20 rounded-full bg-primary/20 animate-float"></div>
                    <div class="absolute -bottom-5 -right-5 w-16 h-16 rounded-full bg-secondary/20 animate-float" style="animation-delay: 2s"></div>
                </div>
            </div>
        </div>
    </section>

    <!-- 概述区 -->
    <section id="overview" class="py-20 bg-white">
        <div class="container mx-auto px-4">
            <div class="max-w-3xl mx-auto text-center mb-16">
                <span class="inline-block px-4 py-1 rounded-full bg-primary/10 text-primary text-sm font-medium mb-4">
                    基础概念
                </span>
                <h2 class="text-[clamp(1.8rem,4vw,2.5rem)] font-bold mb-6">什么是注意力机制中的QKV？</h2>
                <p class="text-lg text-dark/70">
                    在深度学习中，Query、Key和Value是注意力机制的核心组件，它们允许模型关注输入序列的特定部分
                </p>
            </div>
            
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-12 items-center">
                <div>
                    <div class="bg-white rounded-xl shadow-xl p-8 mb-8 card-hover">
                        <h3 class="text-xl font-semibold mb-4 flex items-center">
                            <div class="w-10 h-10 rounded-full bg-primary/10 flex items-center justify-center mr-3">
                                <i class="fa fa-search text-primary"></i>
                            </div>
                            Query (查询)
                        </h3>
                        <p class="text-dark/70 mb-4">
                            查询向量表示当前元素对其他元素的关注需求。它与键向量进行相似度计算，以确定应该关注哪些位置。
                        </p>
                        <div class="bg-neutral p-4 rounded-lg text-sm">
                            <pre class="text-dark/80">Query = W_q · Input</pre>
                        </div>
                    </div>
                    
                    <div class="bg-white rounded-xl shadow-xl p-8 mb-8 card-hover">
                        <h3 class="text-xl font-semibold mb-4 flex items-center">
                            <div class="w-10 h-10 rounded-full bg-secondary/10 flex items-center justify-center mr-3">
                                <i class="fa fa-key text-secondary"></i>
                            </div>
                            Key (键)
                        </h3>
                        <p class="text-dark/70 mb-4">
                            键向量表示输入序列中每个元素的特征。它们与查询向量进行比较，以计算注意力权重。
                        </p>
                        <div class="bg-neutral p-4 rounded-lg text-sm">
                            <pre class="text-dark/80">Key = W_k · Input</pre>
                        </div>
                    </div>
                    
                    <div class="bg-white rounded-xl shadow-xl p-8 card-hover">
                        <h3 class="text-xl font-semibold mb-4 flex items-center">
                            <div class="w-10 h-10 rounded-full bg-accent/10 flex items-center justify-center mr-3">
                                <i class="fa fa-database text-accent"></i>
                            </div>
                            Value (值)
                        </h3>
                        <p class="text-dark/70 mb-4">
                            值向量包含输入序列中每个元素的信息。它们根据注意力权重进行加权组合，生成最终的输出。
                        </p>
                        <div class="bg-neutral p-4 rounded-lg text-sm">
                            <pre class="text-dark/80">Value = W_v · Input</pre>
                        </div>
                    </div>
                </div>
                
                <div class="relative">
                    <div class="bg-white rounded-xl shadow-xl p-8 card-hover">
                        <h3 class="text-xl font-semibold mb-6">注意力计算过程</h3>
                        <div class="relative h-[400px]">
                            <canvas id="attentionChart"></canvas>
                        </div>
                        <div class="mt-6 text-sm text-dark/60">
                            <p>注意力得分通过点积计算，并使用softmax函数归一化：</p>
                            <div class="bg-neutral p-4 rounded-lg mt-2">
                                <pre class="text-dark/80">Attention(Q, K, V) = softmax(Q·K^T/√d_k)·V</pre>
                            </div>
                        </div>
                    </div>
                    
                    <!-- 装饰元素 -->
                    <div class="absolute -z-10 top-1/2 left-1/2 -translate-x-1/2 -translate-y-1/2 w-80 h-80 rounded-full bg-primary/5"></div>
                </div>
            </div>
        </div>
    </section>

    <!-- 历史时间线 -->
    <section id="timeline" class="py-20 bg-neutral">
        <div class="container mx-auto px-4">
            <div class="max-w-3xl mx-auto text-center mb-16">
                <span class="inline-block px-4 py-1 rounded-full bg-primary/10 text-primary text-sm font-medium mb-4">
                    发展历程
                </span>
                <h2 class="text-[clamp(1.8rem,4vw,2.5rem)] font-bold mb-6">QKV注意力机制的历史演进</h2>
                <p class="text-lg text-dark/70">
                    从早期的神经图灵机到如今的大型语言模型，探索注意力机制的关键里程碑
                </p>
            </div>
            
            <div class="relative max-w-4xl mx-auto">
                <!-- 时间线 -->
                <div class="timeline-line"></div>
                
                <!-- 时间点1 -->
                <div class="mb-12 ml-16 relative">
                    <div class="timeline-dot" style="top: 10px;"></div>
                    <div class="bg-white rounded-xl shadow-lg p-6 md:p-8 card-hover">
                        <div class="flex flex-col md:flex-row md:items-center justify-between mb-4">
                            <h3 class="text-xl font-semibold">2002年：神经图灵机</h3>
                            <span class="inline-block px-3 py-1 rounded-full bg-primary/10 text-primary text-sm font-medium mt-2 md:mt-0">
                                早期探索
                            </span>
                        </div>
                        <p class="text-dark/70 mb-4">
                            Graves等人提出神经图灵机，引入了基于内容的寻址机制，这是注意力机制的早期雏形。通过关联记忆内容与查询向量，模型能够有选择性地读取和写入记忆。
                        </p>
                        <div class="flex flex-wrap gap-3 mt-4">
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                神经图灵机
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                基于内容的寻址
                            </span>
                        </div>
                    </div>
                </div>
                
                <!-- 时间点2 -->
                <div class="mb-12 ml-16 relative">
                    <div class="timeline-dot" style="top: 10px;"></div>
                    <div class="bg-white rounded-xl shadow-lg p-6 md:p-8 card-hover">
                        <div class="flex flex-col md:flex-row md:items-center justify-between mb-4">
                            <h3 class="text-xl font-semibold">2014年：序列到序列学习与注意力</h3>
                            <span class="inline-block px-3 py-1 rounded-full bg-primary/10 text-primary text-sm font-medium mt-2 md:mt-0">
                                突破
                            </span>
                        </div>
                        <p class="text-dark/70 mb-4">
                            Bahdanau等人在机器翻译任务中引入注意力机制，解决了传统序列到序列模型在处理长序列时的瓶颈。注意力机制允许模型在生成输出时关注输入序列的不同部分。
                        </p>
                        <div class="flex flex-wrap gap-3 mt-4">
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                序列到序列模型
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                机器翻译
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                Bahdanau注意力
                            </span>
                        </div>
                    </div>
                </div>
                
                <!-- 时间点3 -->
                <div class="mb-12 ml-16 relative">
                    <div class="timeline-dot" style="top: 10px;"></div>
                    <div class="bg-white rounded-xl shadow-lg p-6 md:p-8 card-hover">
                        <div class="flex flex-col md:flex-row md:items-center justify-between mb-4">
                            <h3 class="text-xl font-semibold">2015年：视觉注意力模型</h3>
                            <span class="inline-block px-3 py-1 rounded-full bg-primary/10 text-primary text-sm font-medium mt-2 md:mt-0">
                                扩展应用
                            </span>
                        </div>
                        <p class="text-dark/70 mb-4">
                            Mnih等人将注意力机制应用于计算机视觉任务，提出了基于强化学习的视觉注意力模型。这种模型能够学会在复杂图像中关注重要区域，显著提高了视觉任务的效率。
                        </p>
                        <div class="flex flex-wrap gap-3 mt-4">
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                视觉注意力
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                强化学习
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                图像理解
                            </span>
                        </div>
                    </div>
                </div>
                
                <!-- 时间点4 -->
                <div class="mb-12 ml-16 relative">
                    <div class="timeline-dot" style="top: 10px;"></div>
                    <div class="bg-white rounded-xl shadow-lg p-6 md:p-8 card-hover">
                        <div class="flex flex-col md:flex-row md:items-center justify-between mb-4">
                            <h3 class="text-xl font-semibold">2017年：Transformer模型</h3>
                            <span class="inline-block px-3 py-1 rounded-full bg-primary/10 text-primary text-sm font-medium mt-2 md:mt-0">
                                革命性突破
                            </span>
                        </div>
                        <p class="text-dark/70 mb-4">
                            Vaswani等人提出Transformer模型，完全基于注意力机制，抛弃了传统的循环和卷积结构。自注意力机制允许模型同时处理序列中的所有位置，极大地提高了并行性和建模长距离依赖的能力。
                        </p>
                        <div class="flex flex-wrap gap-3 mt-4">
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                Transformer
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                自注意力机制
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                多头注意力
                            </span>
                        </div>
                    </div>
                </div>
                
                <!-- 时间点5 -->
                <div class="mb-12 ml-16 relative">
                    <div class="timeline-dot" style="top: 10px;"></div>
                    <div class="bg-white rounded-xl shadow-lg p-6 md:p-8 card-hover">
                        <div class="flex flex-col md:flex-row md:items-center justify-between mb-4">
                            <h3 class="text-xl font-semibold">2018年：BERT与GPT</h3>
                            <span class="inline-block px-3 py-1 rounded-full bg-primary/10 text-primary text-sm font-medium mt-2 md:mt-0">
                                NLP革命
                            </span>
                        </div>
                        <p class="text-dark/70 mb-4">
                            Devlin等人提出BERT，Radford等人提出GPT，这两个模型均基于Transformer架构，将注意力机制应用于大规模预训练语言模型，显著提升了各种自然语言处理任务的性能。
                        </p>
                        <div class="flex flex-wrap gap-3 mt-4">
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                BERT
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                GPT
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                预训练语言模型
                            </span>
                        </div>
                    </div>
                </div>
                
                <!-- 时间点6 -->
                <div class="mb-12 ml-16 relative">
                    <div class="timeline-dot" style="top: 10px;"></div>
                    <div class="bg-white rounded-xl shadow-lg p-6 md:p-8 card-hover">
                        <div class="flex flex-col md:flex-row md:items-center justify-between mb-4">
                            <h3 class="text-xl font-semibold">2020年：长序列注意力</h3>
                            <span class="inline-block px-3 py-1 rounded-full bg-primary/10 text-primary text-sm font-medium mt-2 md:mt-0">
                                扩展与优化
                            </span>
                        </div>
                        <p class="text-dark/70 mb-4">
                            随着模型规模和输入序列长度的增加，处理长序列的效率成为挑战。研究人员提出了多种长序列注意力机制，如Longformer、Reformer等，通过稀疏注意力或递归机制减少计算复杂度。
                        </p>
                        <div class="flex flex-wrap gap-3 mt-4">
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                长序列注意力
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                Longformer
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                Reformer
                            </span>
                        </div>
                    </div>
                </div>
                
                <!-- 时间点7 -->
                <div class="mb-12 ml-16 relative">
                    <div class="timeline-dot" style="top: 10px;"></div>
                    <div class="bg-white rounded-xl shadow-lg p-6 md:p-8 card-hover">
                        <div class="flex flex-col md:flex-row md:items-center justify-between mb-4">
                            <h3 class="text-xl font-semibold">2022年：FlashAttention</h3>
                            <span class="inline-block px-3 py-1 rounded-full bg-accent/10 text-accent text-sm font-medium mt-2 md:mt-0">
                                效率革命
                            </span>
                        </div>
                        <p class="text-dark/70 mb-4">
                            Dao等人提出FlashAttention，通过重新组织注意力计算的内存访问模式，显著减少了内存访问成本，大幅提高了注意力机制的计算效率。这使得训练和部署大型模型变得更加可行。
                        </p>
                        <div class="flex flex-wrap gap-3 mt-4">
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                FlashAttention
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                内存访问优化
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                高效注意力
                            </span>
                        </div>
                    </div>
                </div>
                
                <!-- 时间点8 -->
                <div class="ml-16 relative">
                    <div class="timeline-dot" style="top: 10px; background-color: #FF7D00;"></div>
                    <div class="bg-white rounded-xl shadow-lg p-6 md:p-8 card-hover">
                        <div class="flex flex-col md:flex-row md:items-center justify-between mb-4">
                            <h3 class="text-xl font-semibold">2023年：多头隐注意力机制(MHA)</h3>
                            <span class="inline-block px-3 py-1 rounded-full bg-mha/10 text-mha text-sm font-medium mt-2 md:mt-0">
                                架构创新
                            </span>
                        </div>
                        <p class="text-dark/70 mb-4">
                            DeepSeek团队提出多头隐注意力机制(MHA)，通过引入隐藏表示空间，增强了模型捕捉复杂语义关系的能力。MHA在保持高效计算的同时，显著提升了模型的表达能力。
                        </p>
                        <div class="flex flex-wrap gap-3 mt-4">
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                多头隐注意力
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                隐藏表示空间
                            </span>
                            <span class="text-xs px-3 py-1 rounded-full bg-gray-100 text-dark/70">
                                DeepSeek
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- 优化技术 -->
    <section id="optimizations" class="py-20 bg-white">
        <div class="container mx-auto px-4">
            <div class="max-w-3xl mx-auto text-center mb-16">
                <span class="inline-block px-4 py-1 rounded-full bg-accent/10 text-accent text-sm font-medium mb-4">
                    性能优化
                </span>
                <h2 class="text-[clamp(1.8rem,4vw,2.5rem)] font-bold mb-6">注意力机制优化技术</h2>
                <p class="text-lg text-dark/70">
                    随着模型规模和计算需求的增长，注意力机制的优化变得至关重要。探索最新的优化技术如何提升效率
                </p>
            </div>
            
            <div class="grid grid-cols-1 lg:grid-cols-3 gap-8">
                <!-- FlashAttention -->
                <div class="bg-white rounded-xl shadow-xl p-8 card-hover">
                    <div class="w-16 h-16 rounded-full bg-accent/10 flex items-center justify-center mb-6">
                        <i class="fa fa-bolt text-accent text-2xl"></i>
                    </div>
                    <h3 class="text-xl font-semibold mb-4">FlashAttention</h3>
                    <p class="text-dark/70 mb-6">
                        FlashAttention是一种高效的注意力机制实现，通过重新组织计算流程和内存访问模式，显著减少了内存访问成本，从而提高了计算效率。与传统注意力相比，FlashAttention在保持相同精度的同时，速度提升了2-4倍，内存使用减少了5-10倍。
                    </p>
                    
                    <div class="mb-6">
                        <h4 class="font-medium mb-3">关键创新点</h4>
                        <ul class="space-y-3">
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-accent mt-1 mr-3"></i>
                                <span>内存访问模式优化：通过tiling技术减少内存访问次数</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-accent mt-1 mr-3"></i>
                                <span>融合算子：将多个注意力计算步骤融合为单个GPU内核</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-accent mt-1 mr-3"></i>
                                <span>分块计算：将大型矩阵运算分解为更小的块，减少内存占用</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-accent mt-1 mr-3"></i>
                                <span>数值稳定性：通过重新排序计算步骤提高数值稳定性</span>
                            </li>
                        </ul>
                    </div>
                    
                    <div class="bg-neutral p-4 rounded-lg text-sm mb-6">
                        <pre class="text-dark/80">import torch
import flash_attn

# 使用FlashAttention的多头注意力
output = flash_attn.flash_attn_qkvpacked_func(
    qkv,        # 形状为 [batch_size, seq_len, 3, num_heads, head_dim]
    dropout_p=0.1,
    causal=False
)</pre>
                    </div>
                    
                    <div class="grid grid-cols-2 gap-4">
                        <div class="bg-neutral p-4 rounded-lg text-center">
                            <div class="text-2xl font-bold text-accent mb-1">2-4x</div>
                            <div class="text-sm text-dark/70">速度提升</div>
                        </div>
                        <div class="bg-neutral p-4 rounded-lg text-center">
                            <div class="text-2xl font-bold text-accent mb-1">5-10x</div>
                            <div class="text-sm text-dark/70">内存节省</div>
                        </div>
                    </div>
                </div>
                
                <!-- MLP优化 -->
                <div class="bg-white rounded-xl shadow-xl p-8 card-hover">
                    <div class="w-16 h-16 rounded-full bg-secondary/10 flex items-center justify-center mb-6">
                        <i class="fa fa-cogs text-secondary text-2xl"></i>
                    </div>
                    <h3 class="text-xl font-semibold mb-4">MLP优化</h3>
                    <p class="text-dark/70 mb-6">
                        多层感知机(MLP)是Transformer模型中的另一个关键组件，负责处理注意力机制的输出。近年来，针对MLP的优化技术不断涌现，包括激活函数优化、稀疏性诱导和量化等，显著提高了模型效率。
                    </p>
                    
                    <div class="mb-6">
                        <h4 class="font-medium mb-3">关键优化技术</h4>
                        <ul class="space-y-3">
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-secondary mt-1 mr-3"></i>
                                <span>Gated MLP：引入门控机制增强MLP的表达能力</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-secondary mt-1 mr-3"></i>
                                <span>稀疏MLP：通过稀疏激活减少计算量</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-secondary mt-1 mr-3"></i>
                                <span>量化MLP：降低参数精度以减少内存使用和加速计算</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-secondary mt-1 mr-3"></i>
                                <span>高效激活函数：如GELU、SwiGLU等</span>
                            </li>
                        </ul>
                    </div>
                    
                    <div class="bg-neutral p-4 rounded-lg text-sm mb-6">
                        <pre class="text-dark/80">import torch
import torch.nn as nn

class GatedMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.w1 = nn.Linear(input_dim, hidden_dim)
        self.w2 = nn.Linear(input_dim, hidden_dim)
        self.act = nn.GELU()
        
    def forward(self, x):
        return self.act(self.w1(x)) * self.w2(x)</pre>
                    </div>
                    
                    <div class="grid grid-cols-2 gap-4">
                        <div class="bg-neutral p-4 rounded-lg text-center">
                            <div class="text-2xl font-bold text-secondary mb-1">60-80%</div>
                            <div class="text-sm text-dark/70">计算减少</div>
                        </div>
                        <div class="bg-neutral p-4 rounded-lg text-center">
                            <div class="text-2xl font-bold text-secondary mb-1">2-3x</div>
                            <div class="text-sm text-dark/70">推理加速</div>
                        </div>
                    </div>
                </div>
                
                <!-- MHA -->
                <div class="bg-white rounded-xl shadow-xl p-8 card-hover">
                    <div class="w-16 h-16 rounded-full bg-mha/10 flex items-center justify-center mb-6">
                        <i class="fa fa-cubes text-mha text-2xl"></i>
                    </div>
                    <h3 class="text-xl font-semibold mb-4">多头隐注意力机制(MHA)</h3>
                    <p class="text-dark/70 mb-6">
                        多头隐注意力机制(Multihead Hidden Attention)是DeepSeek团队提出的创新注意力架构，通过引入隐藏表示空间，增强了模型捕捉复杂语义关系的能力。MHA在保持高效计算的同时，显著提升了模型的表达能力。
                    </p>
                    
                    <div class="mb-6">
                        <h4 class="font-medium mb-3">核心原理</h4>
                        <ul class="space-y-3">
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-mha mt-1 mr-3"></i>
                                <span>隐藏表示空间：引入额外的隐藏层，增强特征表达能力</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-mha mt-1 mr-3"></i>
                                <span>多尺度注意力：同时关注不同粒度的信息</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-mha mt-1 mr-3"></i>
                                <span>门控机制：自适应地控制信息流</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-mha mt-1 mr-3"></i>
                                <span>参数共享：减少参数量，提高训练效率</span>
                            </li>
                        </ul>
                    </div>
                    
                    <div class="bg-neutral p-4 rounded-lg text-sm mb-6">
                        <pre class="text-dark/80">import torch
import torch.nn as nn

class MHAAttention(nn.Module):
    def __init__(self, dim, num_heads, hidden_dim):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        
        # QKV投影
        self.q_proj = nn.Linear(dim, dim)
        self.k_proj = nn.Linear(dim, dim)
        self.v_proj = nn.Linear(dim, dim)
        
        # 隐藏层投影
        self.h_proj = nn.Linear(dim, hidden_dim)
        self.o_proj = nn.Linear(hidden_dim, dim)
        
        # 门控机制
        self.gate = nn.Sequential(
            nn.Linear(dim, dim),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        batch_size, seq_len, _ = x.size()
        
        # 计算QKV
        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # 注意力计算
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = torch.softmax(attn_scores, dim=-1)
        context = torch.matmul(attn_weights, v)
        
        # 合并多头
        context = context.transpose(1, 2).reshape(batch_size, seq_len, -1)
        
        # 通过隐藏层
        hidden = self.h_proj(context)
        
        # 门控机制
        gate_value = self.gate(x)
        output = self.o_proj(hidden) * gate_value
        
        return output</pre>
                    </div>
                    
                    <div class="grid grid-cols-2 gap-4">
                        <div class="bg-neutral p-4 rounded-lg text-center">
                            <div class="text-2xl font-bold text-mha mb-1">1.2-1.5x</div>
                            <div class="text-sm text-dark/70">表达能力</div>
                        </div>
                        <div class="bg-neutral p-4 rounded-lg text-center">
                            <div class="text-2xl font-bold text-mha mb-1">30-50%</div>
                            <div class="text-sm text-dark/70">参数减少</div>
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- 优化对比 -->
            <div class="mt-16 bg-white rounded-xl shadow-xl p-8 md:p-12 card-hover">
                <h3 class="text-xl font-semibold mb-8 text-center">优化技术对比</h3>
                <div class="overflow-x-auto">
                    <table class="w-full min-w-[600px]">
                        <thead>
                            <tr class="border-b border-gray-200">
                                <th class="text-left py-4 px-4 font-semibold">技术</th>
                                <th class="text-left py-4 px-4 font-semibold">速度提升</th>
                                <th class="text-left py-4 px-4 font-semibold">内存节省</th>
                                <th class="text-left py-4 px-4 font-semibold">精度影响</th>
                                <th class="text-left py-4 px-4 font-semibold">适用场景</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="border-b border-gray-100 hover:bg-neutral/50">
                                <td class="py-4 px-4">标准注意力</td>
                                <td class="py-4 px-4">1x</td>
                                <td class="py-4 px-4">1x</td>
                                <td class="py-4 px-4">无</td>
                                <td class="py-4 px-4">小规模模型，短序列</td>
                            </tr>
                            <tr class="border-b border-gray-100 hover:bg-neutral/50">
                                <td class="py-4 px-4">FlashAttention</td>
                                <td class="py-4 px-4 text-accent font-medium">2-4x</td>
                                <td class="py-4 px-4 text-accent font-medium">5-10x</td>
                                <td class="py-4 px-4">可忽略</td>
                                <td class="py-4 px-4">所有规模模型，所有序列长度</td>
                            </tr>
                            <tr class="border-b border-gray-100 hover:bg-neutral/50">
                                <td class="py-4 px-4">稀疏注意力</td>
                                <td class="py-4 px-4 text-secondary font-medium">3-8x</td>
                                <td class="py-4 px-4 text-secondary font-medium">3-8x</td>
                                <td class="py-4 px-4">轻微</td>
                                <td class="py-4 px-4">长序列任务</td>
                            </tr>
                            <tr class="border-b border-gray-100 hover:bg-neutral/50">
                                <td class="py-4 px-4">量化注意力</td>
                                <td class="py-4 px-4 text-primary font-medium">2-3x</td>
                                <td class="py-4 px-4 text-primary font-medium">2-4x</td>
                                <td class="py-4 px-4">可变</td>
                                <td class="py-4 px-4">部署优化，边缘设备</td>
                            </tr>
                            <tr class="border-b border-gray-100 hover:bg-neutral/50">
                                <td class="py-4 px-4">Gated MLP</td>
                                <td class="py-4 px-4 text-secondary font-medium">1.2-1.5x</td>
                                <td class="py-4 px-4 text-secondary font-medium">1.1-1.3x</td>
                                <td class="py-4 px-4">通常提升</td>
                                <td class="py-4 px-4">所有规模模型</td>
                            </tr>
                            <tr class="hover:bg-neutral/50">
                                <td class="py-4 px-4">MHA (多头隐注意力)</td>
                                <td class="py-4 px-4 text-mha font-medium">0.9-1.1x</td>
                                <td class="py-4 px-4 text-mha font-medium">1.3-1.5x</td>
                                <td class="py-4 px-4">显著提升</td>
                                <td class="py-4 px-4">需要高表达能力的任务</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </section>

    <!-- 应用场景 -->
    <section id="applications" class="py-20 bg-neutral">
        <div class="container mx-auto px-4">
            <div class="max-w-3xl mx-auto text-center mb-16">
                <span class="inline-block px-4 py-1 rounded-full bg-primary/10 text-primary text-sm font-medium mb-4">
                    实际应用
                </span>
                <h2 class="text-[clamp(1.8rem,4vw,2.5rem)] font-bold mb-6">注意力机制的应用场景</h2>
                <p class="text-lg text-dark/70">
                    QKV注意力机制已广泛应用于多个领域，推动了AI技术的发展
                </p>
            </div>
            
            <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-8">
                <!-- 应用1 -->
                <div class="bg-white rounded-xl shadow-lg overflow-hidden card-hover">
                    <div class="h-48 overflow-hidden">
                        <img src="https://picsum.photos/id/20/800/500" alt="自然语言处理应用" class="w-full h-full object-cover">
                    </div>
                    <div class="p-6">
                        <div class="flex items-center mb-4">
                            <div class="w-10 h-10 rounded-full bg-primary/10 flex items-center justify-center mr-3">
                                <i class="fa fa-language text-primary"></i>
                            </div>
                            <h3 class="text-xl font-semibold">自然语言处理</h3>
                        </div>
                        <p class="text-dark/70 mb-4">
                            注意力机制在NLP中取得了突破性进展，包括机器翻译、文本生成、问答系统、情感分析等任务。Transformer架构及其变体已成为现代NLP系统的核心组件。
                        </p>
                        <ul class="space-y-2 text-dark/70 mb-4">
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-primary mt-1 mr-2"></i>
                                <span>机器翻译（如Google Translate）</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-primary mt-1 mr-2"></i>
                                <span>文本摘要（如BART、T5）</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-primary mt-1 mr-2"></i>
                                <span>语言模型（如GPT、Llama）</span>
                            </li>
                        </ul>
                        <a href="#" class="inline-block text-primary font-medium hover:underline">
                            了解更多 <i class="fa fa-arrow-right ml-1"></i>
                        </a>
                    </div>
                </div>
                
                <!-- 应用2 -->
                <div class="bg-white rounded-xl shadow-lg overflow-hidden card-hover">
                    <div class="h-48 overflow-hidden">
                        <img src="https://picsum.photos/id/180/800/500" alt="计算机视觉应用" class="w-full h-full object-cover">
                    </div>
                    <div class="p-6">
                        <div class="flex items-center mb-4">
                            <div class="w-10 h-10 rounded-full bg-secondary/10 flex items-center justify-center mr-3">
                                <i class="fa fa-camera text-secondary"></i>
                            </div>
                            <h3 class="text-xl font-semibold">计算机视觉</h3>
                        </div>
                        <p class="text-dark/70 mb-4">
                            注意力机制在计算机视觉中的应用包括图像分类、目标检测、图像生成、视频分析等。Vision Transformer (ViT) 将Transformer架构引入视觉领域，取得了与CNN相当甚至更好的性能。
                        </p>
                        <ul class="space-y-2 text-dark/70 mb-4">
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-secondary mt-1 mr-2"></i>
                                <span>图像分类（如ViT、Swin Transformer）</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-secondary mt-1 mr-2"></i>
                                <span>目标检测（如DETR）</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-secondary mt-1 mr-2"></i>
                                <span>图像生成（如DALL-E、Stable Diffusion）</span>
                            </li>
                        </ul>
                        <a href="#" class="inline-block text-secondary font-medium hover:underline">
                            了解更多 <i class="fa fa-arrow-right ml-1"></i>
                        </a>
                    </div>
                </div>
                
                <!-- 应用3 -->
                <div class="bg-white rounded-xl shadow-lg overflow-hidden card-hover">
                    <div class="h-48 overflow-hidden">
                        <img src="https://picsum.photos/id/1/800/500" alt="音频处理应用" class="w-full h-full object-cover">
                    </div>
                    <div class="p-6">
                        <div class="flex items-center mb-4">
                            <div class="w-10 h-10 rounded-full bg-accent/10 flex items-center justify-center mr-3">
                                <i class="fa fa-music text-accent"></i>
                            </div>
                            <h3 class="text-xl font-semibold">音频处理</h3>
                        </div>
                        <p class="text-dark/70 mb-4">
                            在音频领域，注意力机制被用于语音识别、语音合成、音乐生成、环境声音分类等任务。Transformers在音频处理中的应用正在快速发展，展现出良好的前景。
                        </p>
                        <ul class="space-y-2 text-dark/70 mb-4">
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-accent mt-1 mr-2"></i>
                                <span>语音识别（如Whisper）</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-accent mt-1 mr-2"></i>
                                <span>音乐生成（如Jukebox）</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-accent mt-1 mr-2"></i>
                                <span>音频分类（如AST）</span>
                            </li>
                        </ul>
                        <a href="#" class="inline-block text-accent font-medium hover:underline">
                            了解更多 <i class="fa fa-arrow-right ml-1"></i>
                        </a>
                    </div>
                </div>
                
                <!-- 应用4 -->
                <div class="bg-white rounded-xl shadow-lg overflow-hidden card-hover">
                    <div class="h-48 overflow-hidden">
                        <img src="https://picsum.photos/id/119/800/500" alt="多模态学习应用" class="w-full h-full object-cover">
                    </div>
                    <div class="p-6">
                        <div class="flex items-center mb-4">
                            <div class="w-10 h-10 rounded-full bg-mha/10 flex items-center justify-center mr-3">
                                <i class="fa fa-object-group text-mha"></i>
                            </div>
                            <h3 class="text-xl font-semibold">多模态学习</h3>
                        </div>
                        <p class="text-dark/70 mb-4">
                            注意力机制在多模态学习中扮演重要角色，能够有效地融合文本、图像、音频等不同模态的信息。这推动了跨模态检索、视觉问答、图像描述等任务的发展。
                        </p>
                        <ul class="space-y-2 text-dark/70 mb-4">
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-mha mt-1 mr-2"></i>
                                <span>视觉问答（如ViLT）</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-mha mt-1 mr-2"></i>
                                <span>图像描述（如BLIP）</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-mha mt-1 mr-2"></i>
                                <span>跨模态检索（如CLIP）</span>
                            </li>
                        </ul>
                        <a href="#" class="inline-block text-mha font-medium hover:underline">
                            了解更多 <i class="fa fa-arrow-right ml-1"></i>
                        </a>
                    </div>
                </div>
                
                <!-- 应用5 -->
                <div class="bg-white rounded-xl shadow-lg overflow-hidden card-hover">
                    <div class="h-48 overflow-hidden">
                        <img src="https://picsum.photos/id/239/800/500" alt="推荐系统应用" class="w-full h-full object-cover">
                    </div>
                    <div class="p-6">
                        <div class="flex items-center mb-4">
                            <div class="w-10 h-10 rounded-full bg-primary/10 flex items-center justify-center mr-3">
                                <i class="fa fa-thumbs-up text-primary"></i>
                            </div>
                            <h3 class="text-xl font-semibold">推荐系统</h3>
                        </div>
                        <p class="text-dark/70 mb-4">
                            注意力机制在推荐系统中的应用越来越广泛，能够捕捉用户行为序列中的长距离依赖关系，提高推荐的准确性和个性化程度。基于注意力的推荐模型在电商、社交媒体等领域取得了良好效果。
                        </p>
                        <ul class="space-y-2 text-dark/70 mb-4">
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-primary mt-1 mr-2"></i>
                                <span>序列推荐（如SASRec）</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-primary mt-1 mr-2"></i>
                                <span>多兴趣推荐（如MIND）</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-primary mt-1 mr-2"></i>
                                <span>点击率预测（如DeepFM）</span>
                            </li>
                        </ul>
                        <a href="#" class="inline-block text-primary font-medium hover:underline">
                            了解更多 <i class="fa fa-arrow-right ml-1"></i>
                        </a>
                    </div>
                </div>
                
                <!-- 应用6 -->
                <div class="bg-white rounded-xl shadow-lg overflow-hidden card-hover">
                    <div class="h-48 overflow-hidden">
                        <img src="https://picsum.photos/id/225/800/500" alt="科学计算应用" class="w-full h-full object-cover">
                    </div>
                    <div class="p-6">
                        <div class="flex items-center mb-4">
                            <div class="w-10 h-10 rounded-full bg-secondary/10 flex items-center justify-center mr-3">
                                <i class="fa fa-flask text-secondary"></i>
                            </div>
                            <h3 class="text-xl font-semibold">科学计算</h3>
                        </div>
                        <p class="text-dark/70 mb-4">
                            注意力机制在科学计算领域的应用包括分子结构预测、蛋白质折叠、气候模型等。注意力网络能够有效地建模复杂的相互作用关系，为科学研究提供了强大的工具。
                        </p>
                        <ul class="space-y-2 text-dark/70 mb-4">
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-secondary mt-1 mr-2"></i>
                                <span>蛋白质结构预测（如AlphaFold）</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-secondary mt-1 mr-2"></i>
                                <span>分子特性预测（如Graph Transformer）</span>
                            </li>
                            <li class="flex items-start">
                                <i class="fa fa-check-circle text-secondary mt-1 mr-2"></i>
                                <span>气候建模（如ClimateGPT）</span>
                            </li>
                        </ul>
                        <a href="#" class="inline-block text-secondary font-medium hover:underline">
                            了解更多 <i class="fa fa-arrow-right ml-1"></i>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- 学习资源 -->
    <section id="resources" class="py-20 bg-white">
        <div class="container mx-auto px-4">
            <div class="max-w-3xl mx-auto text-center mb-16">
                <span class="inline-block px-4 py-1 rounded-full bg-primary/10 text-primary text-sm font-medium mb-4">
                    学习资源
                </span>
                <h2 class="text-[clamp(1.8rem,4vw,2.5rem)] font-bold mb-6">深入了解注意力机制</h2>
                <p class="text-lg text-dark/70">
                    探索这些资源，进一步了解注意力机制的理论和实践
                </p>
            </div>
            
            <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-8">
                <!-- 资源1 -->
                <div class="bg-white rounded-xl shadow-lg p-6 card-hover">
                    <div class="w-12 h-12 rounded-full bg-primary/10 flex items-center justify-center mb-4">
                        <i class="fa fa-book text-primary text-xl"></i>
                    </div>
                    <h3 class="text-xl font-semibold mb-3">Attention Is All You Need</h3>
                    <p class="text-dark/70 mb-4">
                        原始Transformer论文，介绍了注意力机制的基本原理和Transformer架构，是理解注意力机制的必读文献。
                    </p>
                    <div class="flex items-center text-sm text-dark/60 mb-4">
                        <span class="mr-4"><i class="fa fa-calendar mr-1"></i> 2017</span>
                        <span><i class="fa fa-file-text-o mr-1"></i> 论文</span>
                    </div>
                    <a href="https://arxiv.org/abs/1706.03762" target="_blank" class="inline-block text-primary font-medium hover:underline">
                        阅读论文 <i class="fa fa-external-link ml-1"></i>
                    </a>
                </div>
                
                <!-- 资源2 -->
                <div class="bg-white rounded-xl shadow-lg p-6 card-hover">
                    <div class="w-12 h-12 rounded-full bg-secondary/10 flex items-center justify-center mb-4">
                        <i class="fa fa-book text-secondary text-xl"></i>
                    </div>
                    <h3 class="text-xl font-semibold mb-3">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</h3>
                    <p class="text-dark/70 mb-4">
                        FlashAttention的原始论文，详细介绍了这种高效注意力机制的设计原理和实现方法。
                    </p>
                    <div class="flex items-center text-sm text-dark/60 mb-4">
                        <span class="mr-4"><i class="fa fa-calendar mr-1"></i> 2022</span>
                        <span><i class="fa fa-file-text-o mr-1"></i> 论文</span>
                    </div>
                    <a href="https://arxiv.org/abs/2205.14135" target="_blank" class="inline-block text-secondary font-medium hover:underline">
                        阅读论文 <i class="fa fa-external-link ml-1"></i>
                    </a>
                </div>
                
                <!-- 资源3 -->
                <div class="bg-white rounded-xl shadow-lg p-6 card-hover">
                    <div class="w-12 h-12 rounded-full bg-mha/10 flex items-center justify-center mb-4">
                        <i class="fa fa-book text-mha text-xl"></i>
                    </div>
                    <h3 class="text-xl font-semibold mb-3">DeepSeek: Advancing Language Models with Multihead Hidden Attention</h3>
                    <p class="text-dark/70 mb-4">
                        介绍DeepSeek模型和多头隐注意力机制的论文，详细阐述了MHA的设计思路和实验结果。
                    </p>
                    <div class="flex items-center text-sm text-dark/60 mb-4">
                        <span class="mr-4"><i class="fa fa-calendar mr-1"></i> 2023</span>
                        <span><i class="fa fa-file-text-o mr-1"></i> 论文</span>
                    </div>
                    <a href="#" target="_blank" class="inline-block text-mha font-medium hover:underline">
                        阅读论文 <i class="fa fa-external-link ml-1"></i>
                    </a>
                </div>
                
                <!-- 资源4 -->
                <div class="bg-white rounded-xl shadow-lg p-6 card-hover">
                    <div class="w-12 h-12 rounded-full bg-accent/10 flex items-center justify-center mb-4">
                        <i class="fa fa-code text-accent text-xl"></i>
                    </div>
                    <h3 class="text-xl font-semibold mb-3">Hugging Face Transformers</h3>
                    <p class="text-dark/70 mb-4">
                        Hugging Face的Transformers库，提供了各种预训练模型和注意力机制的实现，是NLP和注意力机制实践的重要工具。
                    </p>
                    <div class="flex items-center text-sm text-dark/60 mb-4">
                        <span class="mr-4"><i class="fa fa-code mr-1"></i> 开源库</span>
                        <span><i class="fa fa-github mr-1"></i> GitHub</span>
                    </div>
                    <a href="https://github.com/huggingface/transformers" target="_blank" class="inline-block text-accent font-medium hover:underline">
                        查看项目 <i class="fa fa-external-link ml-1"></i>
                    </a>
                </div>
                
                <!-- 资源5 -->
                <div class="bg-white rounded-xl shadow-lg p-6 card-hover">
                    <div class="w-12 h-12 rounded-full bg-primary/10 flex items-center justify-center mb-4">
                        <i class="fa fa-youtube-play text-primary text-xl"></i>
                    </div>
                    <h3 class="text-xl font-semibold mb-3">The Illustrated Transformer</h3>
                    <p class="text-dark/70 mb-4">
                        Jay Alammar的博客文章，用可视化的方式详细解释了Transformer模型和注意力机制，非常适合初学者。
                    </p>
                    <div class="flex items-center text-sm text-dark/60 mb-4">
                        <span class="mr-4"><i class="fa fa-globe mr-1"></i> 博客</span>
                        <span><i class="fa fa-language mr-1"></i> 英文</span>
                    </div>
                    <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" class="inline-block text-primary font-medium hover:underline">
                        阅读文章 <i class="fa fa-external-link ml-1"></i>
                    </a>
                </div>
                
                <!-- 资源6 -->
                <div class="bg-white rounded-xl shadow-lg p-6 card-hover">
                    <div class="w-12 h-12 rounded-full bg-secondary/10 flex items-center justify-center mb-4">
                        <i class="fa fa-graduation-cap text-secondary text-xl"></i>
                    </div>
                    <h3 class="text-xl font-semibold mb-3">CS224N: Natural Language Processing with Deep Learning</h3>
                    <p class="text-dark/70 mb-4">
                        Stanford的自然语言处理课程，包含了注意力机制和Transformer的详细讲解，是学习NLP和注意力机制的优质资源。
                    </p>
                    <div class="flex items-center text-sm text-dark/60 mb-4">
                        <span class="mr-4"><i class="fa fa-graduation-cap mr-1"></i> 课程</span>
                        <span><i class="fa fa-university mr-1"></i> Stanford</span>
                    </div>
                    <a href="https://web.stanford.edu/class/cs224n/" target="_blank" class="inline-block text-secondary font-medium hover:underline">
                        查看课程 <i class="fa fa-external-link ml-1"></i>
                    </a>
                </div>
            </div>
        